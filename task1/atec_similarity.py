# D:\1_LNY\code\0_data\NLP-course\task1\ATEC\ATEC.test.data
# -*- coding: utf-8 -*-
"""
ATEC è¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡
æ–¹æ³•å¯¹æ¯”ï¼šTF-IDF vs Word2Vec (å¹³å‡æ± åŒ–)
"""

import os
import jieba
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from gensim.models import Word2Vec
from gensim.models.keyedvectors import KeyedVectors
import logging
import argparse

# è®¾ç½®æ—¥å¿—
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# ========================
# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ========================

def load_stopwords(stopwords_path=None):

    """åŠ è½½åœç”¨è¯è¡¨ã€‚è‹¥æ— æŒ‡å®šè·¯å¾„ï¼Œåˆ™ä½¿ç”¨å†…ç½®å¸¸è§åœç”¨è¯"""
    
    if stopwords_path and os.path.exists(stopwords_path): # æŒ‡å®šäº†è·¯å¾„
        with open(stopwords_path, 'r', encoding='utf-8') as f:
            stopwords = set([line.strip() for line in f if line.strip()])
    else:
        # å†…ç½®ç®€æ˜“ä¸­æ–‡åœç”¨è¯ï¼ˆå®é™…å»ºè®®ä¸‹è½½å“ˆå·¥å¤§/ç™¾åº¦/å·å¤§åœç”¨è¯è¡¨ï¼‰
        stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'é‡Œ', 'æ€ä¹ˆ', 'ä»€ä¹ˆ', 'å‘¢', 'å§', 'å•Š', 'å‘€', 'å—', 'äº†',
            ' ','\t','\n'
        }
    return stopwords

def clean_text(text):
    """ç®€å•æ–‡æœ¬æ¸…æ´—ï¼šå»é™¤å¤šä½™ç©ºæ ¼ç­‰"""
    return text.strip()

def tokenize(text, stopwords):
    """ä¸­æ–‡åˆ†è¯ + å»åœç”¨è¯"""
    words = jieba.lcut(text)
    words = [w.strip() for w in words if w.strip() and w not in stopwords]
    return words

def process_pair(line, stopwords):
    """å¤„ç†ä¸€è¡Œæ•°æ®ï¼šè¿”å› (tokens1, tokens2, label)"""
    parts = line.strip().split('\t')
    if len(parts) != 3:
        return None
    sent1, sent2, label = parts[0], parts[1], int(parts[2])
    tokens1 = tokenize(clean_text(sent1), stopwords)
    tokens2 = tokenize(clean_text(sent2), stopwords)
    return tokens1, tokens2, label

def load_dataset(file_path, stopwords):
    """åŠ è½½æ•°æ®é›†"""
    pairs = []
    labels = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            res = process_pair(line, stopwords)
            if res:
                tokens1, tokens2, label = res
                # ä¿ç•™åŸå§‹å­—ç¬¦ä¸²ç”¨äºTF-IDFï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
                text1 = ' '.join(tokens1)
                text2 = ' '.join(tokens2)
                pairs.append((text1, text2))
                labels.append(label)
    return pairs, labels

# ========================
# 2. TF-IDF æ–¹æ³•
# ========================

def tfidf_similarity(pairs, threshold=0.5):
    """ä½¿ç”¨ TF-IDF + ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ç›¸ä¼¼åº¦"""
    all_texts = [text for pair in pairs for text in pair]  # æ‰€æœ‰å¥å­
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(all_texts)
    
    similarities = []
    n = len(pairs)
    for i in range(n):
        idx1 = i * 2
        idx2 = i * 2 + 1
        sim = cosine_similarity(tfidf_matrix[idx1], tfidf_matrix[idx2])[0][0]
        similarities.append(sim)
    
    # è½¬ä¸ºäºŒåˆ†ç±»ï¼ˆ0/1ï¼‰
    pred_labels = [1 if sim >= threshold else 0 for sim in similarities] # è½¬ä¸ºäºŒåˆ†ç±»
    return pred_labels, similarities

# ========================
# 3. Word2Vec æ–¹æ³•
# ========================

def train_word2vec_from_data(train_file, stopwords, vector_size=100, window=5, min_count=1):
    """ä»è®­ç»ƒé›†è®­ç»ƒ Word2Vec æ¨¡å‹"""
    sentences = []
    with open(train_file, 'r', encoding='utf-8') as f:
        for line in f:
            res = process_pair(line, stopwords)
            if res:
                tokens1, tokens2, _ = res
                sentences.append(tokens1)
                sentences.append(tokens2)
    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4)
    return model

def sentence_vector(tokens, word2vec_model):
    """é€šè¿‡å¹³å‡æ± åŒ–å¾—åˆ°å¥å‘é‡"""
    vectors = []
    for word in tokens:
        if word in word2vec_model.wv:
            vectors.append(word2vec_model.wv[word])
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(word2vec_model.vector_size)

def word2vec_similarity(pairs, word2vec_model, threshold=0.5):
    """ä½¿ç”¨ Word2Vec + ä½™å¼¦ç›¸ä¼¼åº¦"""
    pred_labels = []
    similarities = []
    for text1, text2 in pairs:
        tokens1 = text1.split()
        tokens2 = text2.split()
        vec1 = sentence_vector(tokens1, word2vec_model)
        vec2 = sentence_vector(tokens2, word2vec_model)
        if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:
            sim = 0.0
        else:
            sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        similarities.append(sim)
        pred_labels.append(1 if sim >= threshold else 0)
    return pred_labels, similarities

# ========================
# 4. è¯„ä¼°å‡½æ•°
# ========================

def evaluate(y_true, y_pred, method_name):
    """è®¡ç®— Precision, Recall, F1, Accuracy"""
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    acc = accuracy_score(y_true, y_pred)
    print(f"\nã€{method_name}ã€‘æ€§èƒ½æŒ‡æ ‡:")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    print(f"  F1-Score:  {f1:.4f}")
    print(f"  Accuracy:  {acc:.4f}")
    return {"precision": precision, "recall": recall, "f1": f1, "accuracy": acc}

# ========================
# 5. ä¸»å‡½æ•°
# ========================

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_path', type=str, default=r'D:\1_LNY\code\0_data\NLP-course\task1\ATEC\ATEC.train.data')
    parser.add_argument('--test_path',  type=str, default=r'D:\1_LNY\code\0_data\NLP-course\task1\ATEC\ATEC.test.data')
    parser.add_argument('--valid_path', type=str, default=r'D:\1_LNY\code\0_data\NLP-course\task1\ATEC\ATEC.valid.data')
    parser.add_argument('--stopwords_path', type=str, default=None)  # å¯é€‰
    parser.add_argument('--threshold', type=float, default=0.5) #ä»£è¡¨é˜ˆå€¼ï¼Œç”¨äºç¡®å®šç›¸ä¼¼åº¦
    args = parser.parse_args()

    # åŠ è½½åœç”¨è¯
    stopwords = load_stopwords(args.stopwords_path)

    # åŠ è½½æµ‹è¯•é›†
    print("åŠ è½½æµ‹è¯•é›†...")
    test_pairs, test_labels = load_dataset(args.test_path, stopwords)
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_labels)}")

    # === æ–¹æ³•1: TF-IDF ===
    print("\nã€æ–¹æ³•1ï¼šTF-IDF + ä½™å¼¦ç›¸ä¼¼åº¦ã€‘")
    tfidf_preds, tfidf_sims = tfidf_similarity(test_pairs, threshold=args.threshold)
    tfidf_metrics = evaluate(test_labels, tfidf_preds, "TF-IDF")

    # === æ–¹æ³•2: Word2Vec (ä»è®­ç»ƒé›†è®­ç»ƒ) ===
    print("\nã€æ–¹æ³•2ï¼šWord2Vec (è®­ç»ƒ) + å¹³å‡æ± åŒ– + ä½™å¼¦ç›¸ä¼¼åº¦ã€‘")
    print("æ­£åœ¨è®­ç»ƒ Word2Vec æ¨¡å‹ï¼ˆä»…ç”¨è®­ç»ƒé›†ï¼‰...")
    w2v_model = train_word2vec_from_data(args.train_path, stopwords)
    w2v_preds, w2v_sims = word2vec_similarity(test_pairs, w2v_model, threshold=args.threshold)
    w2v_metrics = evaluate(test_labels, w2v_preds, "Word2Vec")

    # === ç»“æœå¯¹æ¯” ===
    print("\n" + "="*50)
    print("ğŸ“Š æ–¹æ³•å¯¹æ¯” (Threshold = {:.2f})".format(args.threshold))
    print("="*50)
    print(f"{'æ–¹æ³•':<12} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Acc':<10}")
    print("-"*50)
    print(f"{'TF-IDF':<12} {tfidf_metrics['precision']:<10.4f} {tfidf_metrics['recall']:<10.4f} {tfidf_metrics['f1']:<10.4f} {tfidf_metrics['accuracy']:<10.4f}")
    print(f"{'Word2Vec':<12} {w2v_metrics['precision']:<10.4f} {w2v_metrics['recall']:<10.4f} {w2v_metrics['f1']:<10.4f} {w2v_metrics['accuracy']:<10.4f}")

if __name__ == '__main__':
    main()

