# -*- coding: utf-8 -*-
"""
ATEC è¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡ - ä¼˜åŒ–ç‰ˆ + å¯è§†åŒ–
æ–¹æ³•ï¼šTF-IDF vs Word2Vecï¼ˆä»…é™è¿™ä¸¤ç§æ–¹æ³•ï¼‰
"""

import os
import jieba
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import (
    precision_score, recall_score, f1_score, accuracy_score,
    precision_recall_curve, roc_curve, auc, confusion_matrix
)
from gensim.models import Word2Vec
import logging
import argparse

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆé˜²æ­¢ä¸­æ–‡ä¹±ç ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

# ========================
# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ========================

def load_stopwords(stopwords_path=None):
    """åŠ è½½åœç”¨è¯ã€‚ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨æ–‡ä»¶ï¼Œå¦åˆ™ç”¨å†…ç½®"""
    if stopwords_path and os.path.exists(stopwords_path):
        with open(stopwords_path, 'r', encoding='utf-8') as f:
            stopwords = set(line.strip() for line in f if line.strip())
    else:
        # æ‰©å±•åœç”¨è¯ï¼ˆå¯æ›¿æ¢ä¸ºå®Œæ•´åœç”¨è¯è¡¨ï¼‰
        stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'é‡Œ', 'æ€ä¹ˆ', 'ä»€ä¹ˆ', 'å‘¢', 'å§', 'å•Š', 'å‘€', 'å—', 'äº†',
            ' ', '\t', '\n', 'ï¼Œ', 'ã€‚', 'ï¼›', 'ï¼š', 'ï¼Ÿ', 'ï¼', 'â€œ', 'â€', 'ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘'
        }
    return stopwords

def clean_text(text):
    return text.strip()

def tokenize(text, stopwords):
    words = jieba.lcut(text)
    return [w.strip() for w in words if w.strip() and w not in stopwords and len(w) > 1]

def process_pair(line, stopwords):
    parts = line.strip().split('\t')
    if len(parts) != 3:
        return None
    sent1, sent2, label = parts[0], parts[1], int(parts[2])
    tokens1 = tokenize(clean_text(sent1), stopwords)
    tokens2 = tokenize(clean_text(sent2), stopwords)
    return tokens1, tokens2, label

def load_dataset(file_path, stopwords):
    pairs = []
    labels = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            res = process_pair(line, stopwords)
            if res:
                tokens1, tokens2, label = res
                text1 = ' '.join(tokens1)
                text2 = ' '.join(tokens2)
                pairs.append((text1, text2))
                labels.append(label)
    return pairs, labels

# ========================
# 2. TF-IDF æ–¹æ³•ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
# ========================

def tfidf_similarity_optimized(pairs, threshold=0.5):
    """TF-IDF + ä¼˜åŒ–å‚æ•°"""
    all_texts = [text for pair in pairs for text in pair]
    # ä¼˜åŒ–å‚æ•°ï¼šngram, sublinear_tf, max_features
    vectorizer = TfidfVectorizer(
        ngram_range=(1, 2),
        sublinear_tf=True,
        max_features=10000
    )
    tfidf_matrix = vectorizer.fit_transform(all_texts)
    similarities = []
    n = len(pairs)
    for i in range(n):
        idx1 = i * 2
        idx2 = i * 2 + 1
        sim = cosine_similarity(tfidf_matrix[idx1], tfidf_matrix[idx2])[0][0]
        similarities.append(sim)
    pred_labels = [1 if sim >= threshold else 0 for sim in similarities]
    return pred_labels, similarities

def find_best_threshold(y_true, similarities, method_name="Method"):
    """è‡ªåŠ¨æœç´¢æœ€ä½³ F1 é˜ˆå€¼"""
    thresholds = np.arange(0.0, 1.01, 0.01)
    best_f1 = 0
    best_thresh = 0.5
    for t in thresholds:
        y_pred = [1 if s >= t else 0 for s in similarities]
        f1 = f1_score(y_true, y_pred)
        if f1 > best_f1:
            best_f1 = f1
            best_thresh = t
    print(f"ã€{method_name}ã€‘æœ€ä¼˜é˜ˆå€¼: {best_thresh:.2f}, å¯¹åº” F1: {best_f1:.4f}")
    return best_thresh, best_f1

# ========================
# 3. Word2Vec æ–¹æ³•ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
# ========================

def train_word2vec_optimized(train_file, stopwords, vector_size=200, window=7, min_count=2):
    """ä¼˜åŒ– Word2Vec è®­ç»ƒå‚æ•°"""
    sentences = []
    with open(train_file, 'r', encoding='utf-8') as f:
        for line in f:
            res = process_pair(line, stopwords)
            if res:
                tokens1, tokens2, _ = res
                sentences.append(tokens1)
                sentences.append(tokens2)
    model = Word2Vec(
        sentences,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        workers=4,
        sg=0  # CBOW
    )
    return model

def sentence_vector(tokens, wv, vector_size):
    vectors = [wv[word] for word in tokens if word in wv]
    if vectors:
        vec = np.mean(vectors, axis=0)
        # å¯é€‰ï¼šL2 å½’ä¸€åŒ–ï¼ˆæå‡ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        norm = np.linalg.norm(vec)
        if norm > 0:
            vec = vec / norm
        return vec
    else:
        return np.zeros(vector_size)

def word2vec_similarity_optimized(pairs, w2v_model, threshold=0.5):
    wv = w2v_model.wv
    vector_size = w2v_model.vector_size
    similarities = []
    for text1, text2 in pairs:
        tokens1 = text1.split()
        tokens2 = text2.split()
        vec1 = sentence_vector(tokens1, wv, vector_size)
        vec2 = sentence_vector(tokens2, wv, vector_size)
        if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:
            sim = 0.0
        else:
            sim = np.dot(vec1, vec2)  # å·²å½’ä¸€åŒ–ï¼Œå³ä½™å¼¦ç›¸ä¼¼åº¦
        similarities.append(sim)
    pred_labels = [1 if sim >= threshold else 0 for sim in similarities]
    return pred_labels, similarities

# ========================
# 4. å¯è§†åŒ–å‡½æ•°
# ========================

def plot_similarity_histogram(y_true, sims_tfidf, sims_w2v, save_path="sim_histogram.png"):
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.hist([s for s, y in zip(sims_tfidf, y_true) if y == 1], bins=30, alpha=0.7, label='Positive', color='green')
    plt.hist([s for s, y in zip(sims_tfidf, y_true) if y == 0], bins=30, alpha=0.7, label='Negative', color='red')
    plt.title('TF-IDF ç›¸ä¼¼åº¦åˆ†å¸ƒ')
    plt.xlabel('ç›¸ä¼¼åº¦')
    plt.ylabel('é¢‘æ¬¡')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.hist([s for s, y in zip(sims_w2v, y_true) if y == 1], bins=30, alpha=0.7, label='Positive', color='green')
    plt.hist([s for s, y in zip(sims_w2v, y_true) if y == 0], bins=30, alpha=0.7, label='Negative', color='red')
    plt.title('Word2Vec ç›¸ä¼¼åº¦åˆ†å¸ƒ')
    plt.xlabel('ç›¸ä¼¼åº¦')
    plt.ylabel('é¢‘æ¬¡')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.show()

def plot_pr_roc(y_true, sims_tfidf, sims_w2v, save_path="pr_roc.png"):
    plt.figure(figsize=(12, 5))
    
    # PR Curve
    plt.subplot(1, 2, 1) # å­å›¾
    for name, sims in [("TF-IDF", sims_tfidf), ("Word2Vec", sims_w2v)]:
        precision, recall, _ = precision_recall_curve(y_true, sims) # è®¡ç®—PRæ›²çº¿
        pr_auc = auc(recall, precision) # è®¡ç®—AUC
        plt.plot(recall, precision, label=f'{name} (AUC={pr_auc:.3f})') # ç»˜åˆ¶PRæ›²çº¿
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend()
    
    # ROC Curve
    plt.subplot(1, 2, 2)
    for name, sims in [("TF-IDF", sims_tfidf), ("Word2Vec", sims_w2v)]:
        fpr, tpr, _ = roc_curve(y_true, sims)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'{name} (AUC={roc_auc:.3f})')
    plt.plot([0,1], [0,1], 'k--', alpha=0.5) # ç»˜åˆ¶å¯¹è§’çº¿ï¼Œæ ·å¼ä¸ºè™šçº¿
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.show()

def plot_f1_vs_threshold(y_true, sims_tfidf, sims_w2v, save_path="f1_vs_threshold.png"):
    thresholds = np.arange(0.0, 1.01, 0.01)
    f1_tfidf = []
    f1_w2v = []
    for t in thresholds:
        f1_tfidf.append(f1_score(y_true, [1 if s >= t else 0 for s in sims_tfidf]))
        f1_w2v.append(f1_score(y_true, [1 if s >= t else 0 for s in sims_w2v]))
    
    plt.figure(figsize=(8, 5))
    plt.plot(thresholds, f1_tfidf, label='TF-IDF', marker='o', markevery=20) # æ ·å¼ä¸º
    plt.plot(thresholds, f1_w2v, label='Word2Vec', marker='s', markevery=20)
    plt.xlabel('Threshold')
    plt.ylabel('F1-Score')
    plt.title('F1-Score vs Threshold')
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path, dpi=300)
    plt.show()

def plot_confusion_matrix(y_true, y_pred, method_name, save_path=None):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Similar', 'Similar'], yticklabels=['Not Similar', 'Similar'])
    plt.title(f'æ··æ·†çŸ©é˜µ - {method_name}')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    if save_path:
        plt.savefig(save_path, dpi=300)
    plt.show()

def plot_similarity_scatter(sims_tfidf, sims_w2v, y_true, save_path="sim_scatter.png"):
    plt.figure(figsize=(6,6))
    colors = ['red' if y == 0 else 'green' for y in y_true]
    plt.scatter(sims_tfidf, sims_w2v, c=colors, alpha=0.6, s=10)
    plt.xlabel('TF-IDF ç›¸ä¼¼åº¦')
    plt.ylabel('Word2Vec ç›¸ä¼¼åº¦')
    plt.title('ä¸¤ç§æ–¹æ³•ç›¸ä¼¼åº¦å¯¹æ¯”ï¼ˆçº¢ï¼šè´Ÿä¾‹ï¼Œç»¿ï¼šæ­£ä¾‹ï¼‰')
    plt.grid(True)
    plt.savefig(save_path, dpi=300)
    plt.show()

# ========================
# 5. è¯„ä¼°å‡½æ•°
# ========================

def evaluate_and_visualize(y_true, y_pred, sims, method_name, save_prefix=""):
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    acc = accuracy_score(y_true, y_pred)
    print(f"\nã€{method_name}ã€‘æ€§èƒ½æŒ‡æ ‡:")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    print(f"  F1-Score:  {f1:.4f}")
    print(f"  Accuracy:  {acc:.4f}")
    
    # å¯è§†åŒ–æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(y_true, y_pred, method_name, f"{save_prefix}_cm.png")
    
    return {"precision": precision, "recall": recall, "f1": f1, "accuracy": acc}

# ========================
# 6. ä¸»å‡½æ•°
# ========================

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--train_path', type=str, default=r'D:\1_LNY\code\0_data\NLP-course\task1\ATEC\ATEC.train.data')
    parser.add_argument('--test_path',  type=str, default=r'D:\1_LNY\code\0_data\NLP-course\task1\ATEC\ATEC.test.data')
    parser.add_argument('--valid_path', type=str, default=r'D:\1_LNY\code\0_data\NLP-course\task1\ATEC\ATEC.valid.data')
    parser.add_argument('--stopwords_path', type=str, default=None)
    parser.add_argument('--use_valid_for_threshold', action='store_true', help="ä½¿ç”¨éªŒè¯é›†é€‰é˜ˆå€¼")
    args = parser.parse_args()

    stopwords = load_stopwords(args.stopwords_path)

    print("åŠ è½½æµ‹è¯•é›†...")
    test_pairs, test_labels = load_dataset(args.test_path, stopwords)
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_labels)}")

    # å†³å®šç”¨å“ªä¸ªæ•°æ®é›†æ‰¾é˜ˆå€¼
    if args.use_valid_for_threshold and os.path.exists(args.valid_path):
        print("ä½¿ç”¨éªŒè¯é›†é€‰æ‹©æœ€ä¼˜é˜ˆå€¼...")
        valid_pairs, valid_labels = load_dataset(args.valid_path, stopwords)
        # TF-IDF on valid
        _, valid_sims_tfidf = tfidf_similarity_optimized(valid_pairs)
        best_thresh_tfidf, _ = find_best_threshold(valid_labels, valid_sims_tfidf, "TF-IDF")
        # Word2Vec on valid
        w2v_model = train_word2vec_optimized(args.train_path, stopwords)
        _, valid_sims_w2v = word2vec_similarity_optimized(valid_pairs, w2v_model)
        best_thresh_w2v, _ = find_best_threshold(valid_labels, valid_sims_w2v, "Word2Vec")
    else:
        print("ä½¿ç”¨æµ‹è¯•é›†è‡ªåŠ¨æœç´¢æœ€ä¼˜é˜ˆå€¼")
        # å…ˆè·‘ä¸€éè·å–ç›¸ä¼¼åº¦
        _, test_sims_tfidf = tfidf_similarity_optimized(test_pairs)
        best_thresh_tfidf, _ = find_best_threshold(test_labels, test_sims_tfidf, "TF-IDF")
        w2v_model = train_word2vec_optimized(args.train_path, stopwords)
        _, test_sims_w2v = word2vec_similarity_optimized(test_pairs, w2v_model)
        best_thresh_w2v, _ = find_best_threshold(test_labels, test_sims_w2v, "Word2Vec")

    # === é‡æ–°ç”¨æœ€ä¼˜é˜ˆå€¼é¢„æµ‹ ===
    tfidf_preds, tfidf_sims = tfidf_similarity_optimized(test_pairs, threshold=best_thresh_tfidf)
    w2v_preds, w2v_sims = word2vec_similarity_optimized(test_pairs, w2v_model, threshold=best_thresh_w2v)

    # è¯„ä¼°
    tfidf_metrics = evaluate_and_visualize(test_labels, tfidf_preds, tfidf_sims, "TF-IDF", "tfidf")
    w2v_metrics = evaluate_and_visualize(test_labels, w2v_preds, w2v_sims, "Word2Vec", "w2v")

    # === ç»“æœå¯¹æ¯” ===
    print("\n" + "="*60)
    print(f"ğŸ“Š æ–¹æ³•å¯¹æ¯” (è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é˜ˆå€¼)")
    print("="*60)
    print(f"{'æ–¹æ³•':<12} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Acc':<10}")
    print("-"*60)
    print(f"{'TF-IDF':<12} {tfidf_metrics['precision']:<10.4f} {tfidf_metrics['recall']:<10.4f} {tfidf_metrics['f1']:<10.4f} {tfidf_metrics['accuracy']:<10.4f}")
    print(f"{'Word2Vec':<12} {w2v_metrics['precision']:<10.4f} {w2v_metrics['recall']:<10.4f} {w2v_metrics['f1']:<10.4f} {w2v_metrics['accuracy']:<10.4f}")

    # === å¯è§†åŒ– ===
    plot_similarity_histogram(test_labels, tfidf_sims, w2v_sims, "similarity_histogram.png")
    plot_pr_roc(test_labels, tfidf_sims, w2v_sims, "pr_roc_curves.png")
    plot_f1_vs_threshold(test_labels, tfidf_sims, w2v_sims, "f1_vs_threshold.png")
    plot_similarity_scatter(tfidf_sims, w2v_sims, test_labels, "similarity_scatter.png")

if __name__ == '__main__':
    main()