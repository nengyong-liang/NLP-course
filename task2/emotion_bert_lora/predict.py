import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support, confusion_matrix,
    roc_curve, auc, classification_report
)
from sklearn.preprocessing import label_binarize
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from peft import PeftModel
from config import Config
from data_loader import get_dataloaders, load_data
import textwrap

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆé¿å…ä¸­æ–‡ä¹±ç ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def load_models_and_tokenizer(config: Config):
    """åŠ è½½åŸå§‹æ¨¡å‹ã€å¾®è°ƒæ¨¡å‹å’Œ tokenizer"""
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)
    
    # åŸå§‹æ¨¡å‹ï¼ˆæ—  LoRAï¼‰
    base_model = AutoModelForSequenceClassification.from_pretrained(
        config.model_name,
        num_labels=config.num_labels
    ).to(config.device)
    base_model.eval()
    
    # å¾®è°ƒæ¨¡å‹ï¼ˆLoRAï¼‰
    ft_model = AutoModelForSequenceClassification.from_pretrained(
        config.model_name,
        num_labels=config.num_labels
    )
    ft_model = PeftModel.from_pretrained(ft_model, os.path.join(config.best_model_path, ""))
    ft_model = ft_model.to(config.device)
    ft_model.eval()
    
    return base_model, ft_model, tokenizer

def predict(model, dataloader, config: Config):
    """é€šç”¨é¢„æµ‹å‡½æ•°"""
    all_preds, all_probs, all_labels = [], [], []
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(config.device)
            attention_mask = batch['attention_mask'].to(config.device)
            labels = batch['labels'].cpu().numpy()
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1).cpu().numpy()
            preds = np.argmax(probs, axis=1)
            
            all_preds.extend(preds)
            all_probs.extend(probs)
            all_labels.extend(labels)
    return np.array(all_labels), np.array(all_preds), np.array(all_probs)

def plot_confusion_matrices(y_true, base_preds, ft_preds, save_dir):
    """ç»˜åˆ¶åŸå§‹æ¨¡å‹ vs å¾®è°ƒæ¨¡å‹çš„æ··æ·†çŸ©é˜µå¯¹æ¯”"""
    labels = ["æ¶ˆæ", "ä¸­æ€§", "ç§¯æ"]
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    for i, (preds, title) in enumerate(zip([base_preds, ft_preds], ["åŸå§‹æ¨¡å‹", "LoRA å¾®è°ƒæ¨¡å‹"])):
        cm = confusion_matrix(y_true, preds, labels=[0,1,2])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=labels, yticklabels=labels, ax=axes[i])
        axes[i].set_title(f'{title}\næ··æ·†çŸ©é˜µ', fontsize=14)
        axes[i].set_xlabel('é¢„æµ‹æ ‡ç­¾')
        axes[i].set_ylabel('çœŸå®æ ‡ç­¾')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'confusion_comparison.png'), dpi=300)
    plt.close()

def plot_roc_curve(y_true, base_probs, ft_probs, save_dir):
    """ç»˜åˆ¶å¤šåˆ†ç±» ROC-AUC æ›²çº¿ï¼ˆä¸€å¯¹å¤šï¼‰"""
    y_bin = label_binarize(y_true, classes=[0,1,2])
    n_classes = 3
    colors = ['red', 'green', 'blue']
    labels = ['æ¶ˆæ', 'ä¸­æ€§', 'ç§¯æ']
    
    plt.figure(figsize=(8, 6))
    
    # å¾®è°ƒæ¨¡å‹ ROC
    for i, color, label in zip(range(n_classes), colors, labels):
        fpr, tpr, _ = roc_curve(y_bin[:, i], ft_probs[:, i])
        roc_auc_val = auc(fpr, tpr)
        plt.plot(fpr, tpr, color=color, lw=2,
                 label=f'{label} (LoRA, AUC={roc_auc_val:.3f})')
    
    # åŸå§‹æ¨¡å‹ ROCï¼ˆè™šçº¿ï¼‰
    for i, color in zip(range(n_classes), colors):
        fpr, tpr, _ = roc_curve(y_bin[:, i], base_probs[:, i])
        plt.plot(fpr, tpr, color=color, lw=1, linestyle='--', alpha=0.7)
    
    plt.plot([0, 1], [0, 1], 'k--', lw=1)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å‡æ­£ç‡ (FPR)')
    plt.ylabel('çœŸæ­£ç‡ (TPR)')
    plt.title('å¤šåˆ†ç±» ROC æ›²çº¿ï¼ˆå®çº¿ï¼šå¾®è°ƒï¼›è™šçº¿ï¼šåŸå§‹ï¼‰')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    plt.savefig(os.path.join(save_dir, 'roc_curve.png'), dpi=300)
    plt.close()

def plot_probability_distribution(base_probs, ft_probs, save_dir):
    """ç»˜åˆ¶é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆçœ‹æ¨¡å‹ç½®ä¿¡åº¦ï¼‰"""
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    for i, (probs, title) in enumerate(zip([base_probs, ft_probs], ["åŸå§‹æ¨¡å‹", "LoRA å¾®è°ƒæ¨¡å‹"])):
        max_probs = np.max(probs, axis=1)
        axes[i].hist(max_probs, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        axes[i].set_title(f'{title}ï¼šæœ€å¤§é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ')
        axes[i].set_xlabel('æœ€å¤§é¢„æµ‹æ¦‚ç‡')
        axes[i].set_ylabel('æ ·æœ¬æ•°é‡')
        axes[i].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'prob_dist.png'), dpi=300)
    plt.close()

def save_error_analysis(test_texts, y_true, base_preds, ft_preds, save_path):
    """ä¿å­˜é”™è¯¯æ ·æœ¬åˆ†æï¼ˆå±•ç¤ºå…¸å‹é”™è¯¯ï¼‰"""
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("=== é”™è¯¯æ ·æœ¬åˆ†æï¼ˆä»…å±•ç¤ºå¾®è°ƒæ¨¡å‹ä»å‡ºé”™çš„æ ·æœ¬ï¼‰===\n\n")
        count = 0
        for i, (text, true, base_pred, ft_pred) in enumerate(zip(test_texts, y_true, base_preds, ft_preds)):
            if true != ft_pred and count < 20:  # åªå±•ç¤º 20 ä¸ª
                label_map = {0: "æ¶ˆæ", 1: "ä¸­æ€§", 2: "ç§¯æ"}
                sentence = "".join(text.split())  # è¿˜åŸä¸ºè¿ç»­ä¸­æ–‡
                wrapped = "\n".join(textwrap.wrap(sentence, width=30))
                f.write(f"æ ·æœ¬ #{count+1}:\n")
                f.write(f"  åŸå¥: {wrapped}\n")
                f.write(f"  çœŸå®: {label_map[true]}\n")
                f.write(f"  åŸå§‹æ¨¡å‹é¢„æµ‹: {label_map[base_pred]}\n")
                f.write(f"  å¾®è°ƒæ¨¡å‹é¢„æµ‹: {label_map[ft_pred]}\n")
                f.write("-" * 50 + "\n")
                count += 1

def plot_performance_radar(base_metrics, ft_metrics, save_dir):
    """ç»˜åˆ¶æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾"""
    labels = np.array(['Accuracy', 'Precision', 'Recall', 'F1'])
    base_vals = [base_metrics['acc'], base_metrics['prec'], base_metrics['rec'], base_metrics['f1']]
    ft_vals = [ft_metrics['acc'], ft_metrics['prec'], ft_metrics['rec'], ft_metrics['f1']]
    
    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
    base_vals += base_vals[:1]
    ft_vals += ft_vals[:1]
    angles += angles[:1]
    
    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))
    ax.fill(angles, base_vals, color='red', alpha=0.25, label='åŸå§‹æ¨¡å‹')
    ax.fill(angles, ft_vals, color='blue', alpha=0.25, label='LoRA å¾®è°ƒ')
    ax.plot(angles, base_vals, color='red', linewidth=2)
    ax.plot(angles, ft_vals, color='blue', linewidth=2)
    
    ax.set_yticklabels([])
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels, fontsize=12)
    plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))
    plt.title('æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾', size=16, y=1.08)
    plt.savefig(os.path.join(save_dir, 'performance_radar.png'), dpi=300, bbox_inches='tight')
    plt.close()

def main():
    config = Config()
    config.data_dir = os.path.join(config.data_dir, "")  # å¯æ”¹ä¸ºå…¨é‡æ•°æ®è·¯å¾„
    
    # åŠ è½½æ•°æ®å’Œæ¨¡å‹
    _, _, test_loader, tokenizer = get_dataloaders(config)
    test_texts, _ = load_data(os.path.join(config.data_dir, "test.tsv"))
    
    base_model, ft_model, _ = load_models_and_tokenizer(config)
    
    # é¢„æµ‹
    y_true, base_preds, base_probs = predict(base_model, test_loader, config)
    _, ft_preds, ft_probs = predict(ft_model, test_loader, config)
    
    # è®¡ç®—æŒ‡æ ‡
    def compute_metrics(y_true, y_pred):
        acc = accuracy_score(y_true, y_pred)
        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
        return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1}
    
    base_metrics = compute_metrics(y_true, base_preds)
    ft_metrics = compute_metrics(y_true, ft_preds)
    
    # æ‰“å°æŒ‡æ ‡
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"åŸå§‹æ¨¡å‹ â†’ Acc: {base_metrics['acc']:.4f}, F1: {base_metrics['f1']:.4f}")
    print(f"LoRAå¾®è°ƒ â†’ Acc: {ft_metrics['acc']:.4f}, F1: {ft_metrics['f1']:.4f}")
    
    # å¯è§†åŒ–
    plot_confusion_matrices(y_true, base_preds, ft_preds, config.plot_dir)
    plot_roc_curve(y_true, base_probs, ft_probs, config.plot_dir)
    plot_probability_distribution(base_probs, ft_probs, config.plot_dir)
    plot_performance_radar(base_metrics, ft_metrics, config.plot_dir)
    save_error_analysis(test_texts, y_true, base_preds, ft_preds, 
                        os.path.join(config.plot_dir, "error_samples.txt"))
    
    print(f"âœ… æ‰€æœ‰å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {config.plot_dir}")

if __name__ == "__main__":
    main()